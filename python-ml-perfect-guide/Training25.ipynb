from google.colab import drive
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
%matplotlib inline

drive.mount('/content/gdrive')

titanic_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/titanic_train.csv')
titanic_df.head()

# 사이킷런 머신러닝 알고리즘은 Null 값을 허용하지 않기에 어떻게 처리할지 결정해야한다.
# 먼저 Null 값이 존재하는지 info() 메서드를 통해 확인해야 한다.
# 이후 DataFrame의 fillna() 함수를 사용해 간단하게 평균 또는 고정 값으로 변경한다.
print ('\n ### 학습 데이터 정보 ### \n')

titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace = True)
titanic_df['Cabin'].fillna('N', inplace=True)
titanic_df['Embarked'].fillna('N', inplace=True)
# print ('데이터 셋 Null 값 개수 ', titanic_df.isnull().sum().sum())

print ('[Sex 값 분포]\n', titanic_df['Sex'].value_counts())
print ('\n[Cabin 값 분포]\n', titanic_df['Cabin'].str[:1])
print ('\n[Embarked 값 분포]\n', titanic_df['Embarked'].value_counts())
print ('\n')

####################################################################################

titanic_df.groupby(['Sex','Survived'])['Survived'].count()

# 가로 막대 차트를 그리기 위해서는 barplot() 함수를 호출하면 가능
# sns.barplot(x = 'Sex', y = 'Survived', data= titanic_df)

# hue 파라미터에 카테고리 변수 이름을 지정하여 카테고리 별 색상을 다르게 가능
# sns.barplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = titanic_df)


def get_category(age):
  cat = ''
  if age <= -1: cat = 'Unknown'
  elif age <= 5: cat = 'Baby'
  elif age <= 12: cat = 'Child'
  elif age <= 18: cat = 'Teenager'
  elif age <= 25: cat = 'Student'
  elif age <= 35: cat = 'Young Adult'
  elif age <= 60: cat = 'Adult'
  else : cat = 'Elderly'

  return cat

# 막대그래프의 크기 figure를 더 크게 설정
plt.figure(figsize = (10, 6))

# X축의 값을 순차적으로 표시하기 위한 설정
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']

# lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정.
# get_category(X)는 입력값으로 'Age 칼럼 값을 받아서 해당하는 cat 반환
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))
# sns.barplot(x = 'Age_cat', y = 'Survived', hue = 'Sex', data=titanic_df, order = group_names)
titanic_df.drop('Age_cat', axis=1, inplace=True)

# 결과 값을 확인할 경우 여성 Baby의 생존확류링 비교적 높고, 여성 Child의 경우 다른 연령대에 비해 생존 확률이 낮음
# 또한 여성 Elderly 경우 매우 생존확률이 높음. 이제껏 분석한 결과 Sex, Age, PClass 등이 생존을 중요하게 좌우하는 피처임을 확인가능

####################################################################################

def encode_features(dataDF):
  features = ['Cabin', 'Sex', 'Embarked']
  for feature in features:
    le = preprocessing.LabelEncoder()
    le = le.fit(dataDF[feature])
    dataDF[feature] = le.transform(dataDF[feature])

  return dataDF

# # 결과적으로 Sex, Cabin, Embarked 속성이 숫자형으로 바뀐 것을 확인할 수 있음
titanic_df = encode_features(titanic_df)
titanic_df.head()

####################################################################################

def fillna(df):
  df['Age'].fillna(df['Age'].mean(), inplace = True)
  df['Cabin'].fillna('N', inplace = True)
  df['Embarked'].fillna('N', inplace = True)
  df['Fare'].fillna(0, inplace = True)
  return df

# 머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
  df.drop(['PassengerId', 'Name', 'Ticket'], axis = 1, inplace=True)
  return df

# 레이블 인코딩 수행
def format_features(df):
  df['Cabin'] = df['Cabin'].str[:1]
  features = ['Cabin', 'Sex', 'Embarked']
  for feature in features:
    le = LabelEncoder()
    le = le.fit(df[feature])
    df[feature] = le.transform(df[feature])
  return df

# 앞에서 설정한 데이터 전처리 함수 호출
def transform_features(df):
  df = fillna(df)
  df = drop_features(df)
  df = format_features(df)
  return df

titanic_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/titanic_train.csv')
Y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis = 1)

X_titanic_df = transform_features(X_titanic_df)

X_train, X_test, Y_train, Y_test = train_test_split(X_titanic_df, Y_titanic_df, test_size = 0.2, random_state = 11)


dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression()

dt_clf.fit(X_train, Y_train)
dt_pred = dt_clf.predict(X_test)
print ('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(Y_test, dt_pred)))

rf_clf.fit(X_train, Y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier 정확도: {0:.4f}'.format(accuracy_score(Y_test, rf_pred)))

lr_clf.fit(X_train, Y_train)
lr_pred = lr_clf.predict(X_test)
print ('LogisticRegression 정확도: {0:.4f}\n'.format(accuracy_score(Y_test, lr_pred)))

#########################################################################################

def exec_kfold(clf, folds=5):
  # 폴드 셋을 5개인 KFold 객체를 생성, 폴드 수만큼 예측결과 저장을 위한 리스트 객체 생성.
  kfold = KFold(n_splits=folds)
  scores = list()

  # KFold 교차 검증 수행.
  for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
    X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
    Y_train, Y_test = Y_titanic_df.values[train_index], Y_titanic_df.values[test_index]

    clf.fit(X_train, Y_train)
    predictions = clf.predict(X_test)
    accuracy = accuracy_score(Y_test, predictions)
    scores.append(accuracy)
    print ("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))

  mean_score = np.mean(scores)
  print ("평균 정확도: {0:.4f}\n".format(mean_score))

#exe_kfold 호출
exec_kfold(dt_clf, folds=5)

#########################################################################################
# 교차 검증을 cross_val_score() API를 이용해 수행
# cross_val_score()와 KFold의 평균 정확도가 약간 다른데, 이는
# cross_val_score()가 StratifiedKFold를 이용해 폴드 셋을 분할하기 때문이다.

scores = cross_val_score(dt_clf, X_titanic_df, Y_titanic_df, cv = 5)
for iter_count, accuracy in enumerate(scores):
  print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))

print ("평균 정확도: {0:.4f}\n".format(np.mean(scores)))

#########################################################################################

# GridSearchCV를 이용해 DecisionTreeClassifier의 최적 하이퍼 파라미터를 찾고 예측 성능을 측정
parameters = {'max_depth':[2, 3, 5, 10],
              'min_samples_split':[2, 3, 5], 'min_samples_leaf':[1, 5, 8]}

grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring = 'accuracy', cv=5)
grid_dclf.fit(X_train, Y_train)

print ('\nGridSearchCV 최적 하이퍼 파라미터 :', grid_dclf.best_params_)
print ('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))
best_dclf = grid_dclf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행.
dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(Y_test, dpredictions)
print ('테스트 셋에서의 DecisionTreeClassifier 정확도: {0:.4f}\n'.format(accuracy))

