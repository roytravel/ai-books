{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수치 미분 - 중심 차분 => 오차 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, np.array([5.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # x와 같은 차원의 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n"
     ]
    }
   ],
   "source": [
    "print (_numerical_gradient(function_2, np.array([3.0, 4.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        \n",
    "        grad = _numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x, np.array(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0]) #NN이 시작할 때 초기값을 의미 함. (-3,4)에서 시작함 / 실제로는 랜덤 값을 줌\n",
    "# 초기 값을 주는 방법에도 여러가지가 있음 Xavier, He 등이 있고 ReLU일 때 He를, Sigmoid면 Xavier를 쓰는게 좋다. (논문 기재)\n",
    "\n",
    "lr = 0.1\n",
    "step_num = 100\n",
    "x, x_history = gradient_descent(function_2 ,init_x, lr=lr, step_num = step_num)\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot([-5,5], [0,0])\n",
    "plt.plot([0,0], [-5, 5])\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, aixs=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    \n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규 분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient(f, X)\n",
    "    \n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient(f, x)\n",
    "            \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14667667  0.30597142 -0.45264809]\n",
      " [ 0.22001501  0.45895713 -0.67897213]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print (dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
