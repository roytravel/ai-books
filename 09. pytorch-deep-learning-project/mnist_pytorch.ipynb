{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n",
      "3.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.924133\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.313336\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678430\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477235\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.526729\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.469570\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.243354\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.535540\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.250727\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.449397\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.415974\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.327603\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.501861\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.146283\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.372229\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.093487\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.173131\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.296608\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.086363\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.334908\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.037923\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.368311\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.237165\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.357118\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.204113\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.050533\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.347196\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.222008\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.196307\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.219461\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.371997\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.076640\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.063637\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.239423\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.055383\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.360913\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.023938\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.091675\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.103321\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.190044\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.072944\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.060187\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.073172\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.164674\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.372170\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.078066\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.129855\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.092128\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.164644\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.046490\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.270464\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.052892\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.031950\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.038460\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.019325\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.053178\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.208981\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.077057\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.056482\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.073632\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.118282\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.053564\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.226998\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.257602\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.246156\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.094773\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.073710\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.141951\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.288561\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.080773\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.124614\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.228794\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.145711\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.069470\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.144412\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.027573\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.028125\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.577887\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.449275\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.129446\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.031079\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.155822\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.013211\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.050209\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.129425\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.036181\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.394779\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.117680\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.209140\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.011749\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.054243\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.511668\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.148937\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.253182\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.080742\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.080144\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.024576\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.009845\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.124462\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.091768\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.060347\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.254630\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.187278\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.014570\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.036776\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.004277\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.159036\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.247975\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.165650\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.020769\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.171206\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.095169\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.146039\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.054088\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.192695\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.040132\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.115668\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.052174\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.456471\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.049946\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.156750\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.024614\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.117552\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.071479\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.227584\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.183102\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.206875\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.161943\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.061622\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.010551\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.049372\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.080375\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.046545\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.237970\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.041292\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.135201\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.067930\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.275516\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.019263\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.171098\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004647\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.216872\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.058772\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.101013\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.065428\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.015367\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.186005\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.195530\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.203482\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.013121\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.094128\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.197300\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.301263\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.125687\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.061686\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.004499\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004558\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.249185\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.025598\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.012293\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.012412\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.014274\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.039375\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.016903\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.035044\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.024567\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.005434\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.157171\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.010556\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001412\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.050422\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.071518\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.043187\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.158121\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.061694\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.042585\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.077480\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.003835\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.071643\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.036640\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.006439\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.098434\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.009693\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.029291\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.028656\n",
      "\n",
      "Test dataset: Overall Loss: 0.0479, Overall Accuracy: 9845/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.020359\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.025741\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.022931\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.152773\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.011915\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.167253\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.254765\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.006424\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.161262\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.206669\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.075022\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.005815\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.002276\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.018408\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.005063\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.037720\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.099554\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.052310\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.088934\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.008392\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.023547\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.019402\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.076369\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.022635\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.009572\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.137908\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.242286\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.034721\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.132401\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.155467\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.010599\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.295828\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.004608\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.043506\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.186897\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.021783\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.006585\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.011303\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.015140\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.128739\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.028640\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.006435\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.001083\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.035964\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.018474\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.091950\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.020873\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.002873\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.031803\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.120622\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.143689\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.281560\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.061713\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.032963\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.018278\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.032960\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.128510\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.106541\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.060940\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.025870\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.054985\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.106494\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.009570\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.010299\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.004964\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.022490\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.095111\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.028878\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.109379\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.054732\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.310703\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.116455\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.027312\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.013424\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.083364\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.021839\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.626760\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.005461\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.030631\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.019019\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.028807\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.100890\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.012964\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.003434\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.013302\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.016626\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.123718\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.010024\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.001419\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.228615\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.146616\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.064385\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.001509\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.022064\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.173226\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.171696\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.079744\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.015279\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.037458\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.031818\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.070076\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.071059\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.039097\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.010789\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.095843\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.020878\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.085201\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.205518\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.244873\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.021297\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.045317\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.011094\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.008661\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.038454\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.057592\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.009687\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.007862\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.089237\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.286535\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.233179\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.037464\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.075128\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.008064\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.018365\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.152207\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.060987\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.032562\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.018490\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.061172\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.034900\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.054468\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.001548\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.001852\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.051221\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.066732\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.007792\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.106629\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.093534\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.158624\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.002119\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.283132\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.045434\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.397113\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.009885\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.008022\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.034414\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.001653\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.088995\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.205148\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.076702\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.025029\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.117145\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.001660\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.027944\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.014792\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.010374\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.375141\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.136964\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.110864\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.094381\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.111132\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.067488\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.027945\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.073563\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.006258\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.002482\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.019000\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.040063\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.017319\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.060264\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.004781\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.192581\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.103267\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.017297\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.005782\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.052227\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.005727\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.001461\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.160998\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.145424\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.277233\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.144867\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.048502\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.003514\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.010325\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.004135\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.015244\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.042271\n",
      "\n",
      "Test dataset: Overall Loss: 0.0370, Overall Accuracy: 9871/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 0\n",
      "Ground truth is : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"./convnet.pth\"\n",
    "torch.save(model.state_dict(), PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
